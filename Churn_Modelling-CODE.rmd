---
title: "Churn_Mondelling"
author:
  - Elena Patsalou
  - Natalia Korai
  - Waqar Aziz Sulaiman
date: "`r Sys.Date()`"
output: html_document
---

```{r}
#install.packages('caTools')
#install.packages("klaR")
#install.packages('caret')
#install.packages('ROSE')
#install.packages('naivebayes')

```

```{r}
# Libraries
library(e1071)
library(tidyverse)
library(corrplot)
library(dplyr)
library(ROSE)
library(caret)
library(boot)
library(glmnet)
library(naivebayes)
library(caTools)
library(MASS)
library(pROC)
library(class)
library(partition)
library(klaR)
```
## 1. Read the Data
```{r}
df <- read.csv("C:/Users/ntbna/Downloads/Spring Semester 2023/DSC532/Classification PROJECT/Churn_Modelling.csv")
# 10000 observations and 14 columns
head(df)

# Preprocessing
cat('Duplicates:', nrow(df[duplicated(df), ]),'\n') # check for duplicates rows
cat('Duplilcates in the column CustomerId:', sum(duplicated(df$CustomerId)), '\n') # check for duplicates for ID
cat('Missing Values for each column\n')
sapply(df, function(x) sum(is.na(x))) # check for any missing values

str(df) # check the type of each column
```

## 2.1. Pre-processing

## 2.1.1 Unique Values
```{r}
# number of unique values for each column
number_uniques = c()
for (i in 1:14){
  number_uniques[i] = length(unique(df[,i])) #how many unique values each column has
}
number_uniques = as.data.frame(number_uniques)
number_uniques$col = colnames(df)
number_uniques$index = 1:14
number_uniques
```

## 2.1.2 Drop Columns RowNumber, CustomerID, and Surname

```{r}
df <- df[-c(1,2,3)]
names(df)
```

The only columns which are categorical are Geography and Gender. They will be transformed into dummy variables, since they contain 3 and 2 unique values, respectively

## 2.1.3 Outliers

```{r}
# check for outliers for numerical variables, excluding the sets of {0,1}
find_outliers <- function(x){
  H=1.5 * IQR(x)
  number <- sum(x< (quantile(x)[2]-H)) + sum(x> (quantile(x)[4]+H))
  number
}
df_continuous <- df[,c(1, 4, 6, 10)]
outliers=c()
for (i in 1:4){
  outliers[i]=find_outliers(df_continuous[1:10000,i])
}
# percentage of outliers for each of the above columns
outliers_tois100=as.data.frame(round(((outliers/10000)*100),2)) # Percentage of outliers for each feature
outliers_tois100$col=colnames(df_continuous) # Add the name of each column
outliers_tois100
```

Although we have less than 4% of outliers in column Age we cannot be sure if they are negligible. We can drop them later on.

```{r}
# check for the Age if the values make sense, since there are 70 unique ones; 18 years as the minimun and 92 year as the
# maximum are logic
summary(df['Age'])
```


## 2.1.4 Exploratory Analysis

```{r}
# continuous variables
for (i in 1:4){
  boxplot(df_continuous[,i], ylab = names(df_continuous)[i])
}

# zoom in for the CreditScore to see how far the outliers are
boxplot(df_continuous$CreditScore, ylim=range(200:500),ylab = 'Credit Score')
```

We verify that CreditScore and Age has some outliers; specifically the CreditScore has its outliers lower than the lower whisker, whereas the Age has greater values than the upper whisker. Also, the values of CreditScore are not quite much far from the lower whisker, hence we decide not to drop them. Similarly, the outliers of the Age are not very extreme since they are located close to the whisker.

```{r}
# discrete variables, but not the {0,1}s
library(ggplot2)
ggplot(df, aes(x=Geography)) +
  geom_bar(fill = 'lightblue')
ggplot(df, aes(x=NumOfProducts)) +
  geom_bar(fill = 'lightblue')
par(mfrow=c(1,2))


# Histogram
hist(df$Age, prob = TRUE, col = "white",
    main = "Histogram of Age", xlab = 'Age', ylab = 'Density')
lines(density(df$Age), col = 4, lwd = 2)
hist(df$CreditScore, prob = TRUE, col = "white",
    main = "Histogram of CreditScore", xlab = 'CreditScore', ylab = 'Density')
lines(density(df$CreditScore), col = 4, lwd = 2)

# table of the {0,1} variables
table(df$Gender)
table(df$HasCrCard)
table(df$IsActiveMember)
table(df$Exited)

# check skewness for continuous variables that seem to be skewed
skewness(df$Age)
skewness(df$CreditScore)

# males = 55 %, females = 45 %
# customers with credit card = 70 %
# active customers = 51 %
# exited customers = 20 %
```

* Approximately 2500 customers are from Germany, 2500 from Spain, while the rest 5000 customers are from France.
* The most of customers utilize either 1 or 2 products of the bank (1 product is more common); just over 250 people have 3 products and 60 individuals have 4.
* The 55% of customers of the bank are males and the rest 45% are females. 
* The most of customers (70%) hold a credit card of the bank, whereas 51% are active members.
* 20% of the customers decided to churn.
* The most of customers of the Bank are approximately from 25 to 
50 years old, and few people are older than 60 years, whereas even fewer customers are younger than 25; the distributio of Age is slightly right skewed.

## 2.1.4.1 Comparison between Continuous and Categorical Variables

Credit Score vs [Geography, Gender, NumOfProducts, IsActiveMember, HasCrCard, Exited, Tenure]

```{r}
glimpse(df)
```

We can see that there are many discrete variables that we want to explore integer as their type. That could result in creating one big boxplot for each category, and not multiple. We must change them into characters so the program recognise them as binary values.

```{r}
# Convert discrete variables from integers to characters in order to create boxplots
df$NumOfProducts = as.character(df$NumOfProducts)
df$IsActiveMember = as.character(df$IsActiveMember)
df$HasCrCard = as.character(df$HasCrCard)
df$Exited = as.character(df$Exited)
df$Tenure = as.character(df$Tenure)
```

```{r}
# Create two for loops, that generate boxplots for all categorical variables
# The first for loop is without hue
# The second for loop is with hue
# Take into consideration that boxplots within the big boxplots are ordered from smaller to larger medians, to draw into conclusions more easily

# Create list of variables to iterate over
my_list <- c("Geography", "Gender", "NumOfProducts", "IsActiveMember", "HasCrCard", "Exited", "Tenure")

# First for loop
draw_boxplots <- function(df, y_var, my_list) {
  for (var in my_list) {
    var_ordered <- reorder(df[[var]], df[[y_var]], median)
    df[[var]] <- factor(df[[var]], levels = levels(var_ordered))
    p <- ggplot(df, aes(x = !!sym(var), y = !!sym(y_var))) + geom_boxplot() +
      ggtitle(paste("Plot of", y_var, "by", var))
    print(p)
  }
}

# Second for loop
draw_boxplots1 <- function(df, y_var, my_list) {
  for (var in my_list) {
    var_ordered <- reorder(df[[var]], df[[y_var]], median)
    df[[var]] <- factor(df[[var]], levels = levels(var_ordered))
    p <- ggplot(df, aes(x = !!sym(var), y = !!sym(y_var), fill = factor(Exited))) + geom_boxplot() +
      ggtitle(paste("Plot of", y_var, "by", var))
    print(p)
  }
}
```

*without using 'Exited' as a fill*

```{r}
draw_boxplots(df, "CreditScore", my_list)
```

*Using 'Exited' as a fill*

```{r}
draw_boxplots1(df, "CreditScore", my_list)
```

EstimatedSalary vs [Geography, Gender, NumOfProducts, IsActiveMember, HasCrCard, Exited, Tenure]

*without using 'Exited' as a fill*

```{r}
draw_boxplots(df, "EstimatedSalary", my_list)
```

*Using 'Exited' as a fill*

```{r}
draw_boxplots1(df, "EstimatedSalary", my_list)
```

Balance vs [Geography, Gender, NumOfProducts, IsActiveMember, HasCrCard, Exited, Tenure]

*without using 'Exited' as a fill*

```{r}
draw_boxplots(df, "Balance", my_list)
```

*using 'Exited' as a fill*

```{r}
draw_boxplots1(df, "Balance", my_list)
```

## 2.1.4.2 Comparison between Two Continuous Variables

```{r}
qplot(CreditScore, Balance,  data = df, color = Gender)
qplot(CreditScore, Balance,  data = df, color = Geography)
qplot(CreditScore, Balance,  data = df, color = Exited)
```

Observing the plots of Balance and CreditScore we cannot conclude to a clear statement based on the Geography, Gender and the decision to exit from the bank.

```{r}
qplot(EstimatedSalary, Balance,  data = df, color = HasCrCard)
qplot(EstimatedSalary, Balance,  data = df, color = IsActiveMember)
qplot(EstimatedSalary, Balance,  data = df, color = NumOfProducts)
```

Similarly. observing the above plots of Balance and EstimatedSalary we cannot conclude to a pattern based on the possesion of credit card, whether the customer is an active member, and the number of products they utilise.

## Create dummy variable ##

This is needed for the following plots

```{r}
# The features of Gender and Geography could be tranformed into dummy variables.
df$France <- ifelse(df$Geography == 'France', 1, 0)
df$Germany <- ifelse(df$Geography == 'Germany', 1, 0)
df$Female <- ifelse(df$Gender == 'Female', 1, 0 )
```

```{r}
df <- df[,-c(2,3)]
names(df)
```

## ## 2.1.4.3 Comparison between Two Categorical Variables


```{r}
# visualise the tables (counts of each variable), to understand in a better way than tables
new <- df[,c(5,6,7,9,10,11,12)]


for (i in 1:7){
  for (j in 1:7){
    if ((i != j) & (i < j)){ # in order to have neither pairs (i,i) nor repetitions
      counts <- table(new[,i], new[,j])
      mosaicplot(counts, xlab=names(new)[i], ylab=names(new)[j],
            col=c('lightblue','pink'))
    }
  }
}
```

* NumOfProducts vs
  * HasCreditCard: The most of customers have credit card and the possession is not affected from the number of products which are held by them.
  * IsActiveMember: Active people with 2 items of the bank are more than those who are not active. Moreover, there are less active customers who utilise 3 products than those are not active.
  * Exited: The most of people who hold 3 items decided to exit the bank whereas people who have either 1 or 2 did not exit. Also, all people who possess 4 products have churned. 
  * France: Half of the customers who have 1, or 2, or 4 products are from France whereas the highest percentage of those who utilise 3 items are not French.
  * Germany: In contrary, customers from German constitutes lower percentage on possessing of bank's products. (French are the 50%, while Spanish and Germans are both 25%.)
  * Female: More women utilise either 3 or 4 products in comparison with men.
  
* HasCrCard vs:
  * IsActiveMember: Most of customers have credit card. We have approximately same number of active and not active membes.
  * Exited: The same percentage of customers based on the possession of credit card decided not to churn.

* IsActiveMember vs:
  * Exited: The most of customers who decided to churn were not active members (approximately 2/3).
  * Female: The most of the active members are males.

* Exited vs:
  * France: Although 50% of customers are from France the most of exited ones are from either Germany or Spain.
  * Female: The most of exited customers are women.

* France vs:
  * Female: The most French customers are males. (Overall, the men customers are around 1000 more than women ones) 

* Germany vs:
  * Female: The most German customers are males.
  
## ## 2.1.4.4 Correlation between variables

```{r}
# Convert discrete variables from characters to integers
df$NumOfProducts = as.integer(df$NumOfProducts)
df$IsActiveMember = as.integer(df$IsActiveMember)
df$HasCrCard = as.integer(df$HasCrCard)
df$Exited = as.integer(df$Exited)
df$Tenure = as.integer(df$Tenure)
corrplot(cor(df))
```

* Germany is (positively) correlated with Balance (and negatively with France, due their definition).  
* Balance is correlated with France (negatively), Germany and NumOfProducts (negatively)(Elena: I don't see any correlation between Germany and NumOfProducts). (Natalia: I mean the 1st one (Balance) is correlated with these 3 variables)
* The target variable Exited is correlated with Age, Balance, IsActiveMember(negatively), France(negatively), Germany and Female.
* IsActiveMember is slightely (positively) correlated with Age, which means as the age of a client increases, the higher the possibility is for him to be active with the Bank (Maybe because older people are more interested in saving their money, and more responsible. Also get higher salaries and have the opportunity to be involved with the Bank, buy products or insert money to their account)

```{r}
pairs(df)
```

```{r}
# exclude the binary features (not Exited)
pairs(df[,c(1,2,3,4,5,9)])
```

We cannot conlude to a pattern through the above pair plots, neither to a statement where we had to transform any predictor into another form, such as log or polynomial of order greater than 1.

## 2.1.5 Balancing the Data

A new dataset will be created, that fixes the inbalance between the two classes

## 2.1.5.1 Oversampling

```{r}
oversampled_data <- ovun.sample(Exited ~ ., data = df, method = "over")
# Extract the oversampled data
oversampled_churn_data <- oversampled_data$data

# Check the distribution of classes in the oversampled data
table(oversampled_churn_data$Exited)
#summary(oversampled_churn_data)
```

## 2.1.6 Standardized the Data

We created two different scaled dataframes of the imbalanced dataset, each one for different purposes. All models will use one of those, which fit them the best. We also scale the oversampled dataset.

```{r}
head(df)
```

The range of the columns varies too much hence we have to standardize all columns, in order for the machine learning algoriths to perform better.

First DataFrame - imbalanced
```{r}
df_scaled <-  df %>% mutate_at (c('CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary'), ~(scale(.) %>% as.vector))
head(df_scaled)
```

Second Dataframe - imbalanced
```{r}
df_scaled_1 <- df

# 'Age' variable is right skewed, therefore it may be better to use a scaling method that is less affected by outliers, such as Robust scaling
df_scaled_1$Age <- scale(df$Age, median(df$Age), scale = IQR(df$Age))

# 'EstimatedSalary' variable does not contain any outliers, so it would be best to scale it with Standardization scaling that center the values around 0 and scale them to have a standard deviation of 1
df_scaled_1$EstimatedSalary <- scale(df_scaled_1$EstimatedSalary)

# 'Balance' column shows that 25% of the data lies to the right of the upper quartile value, it suggests that the distribution of the data is skewed to the right. In this case, it may be appropriate to use a scaling method that is robust to skewness, 
# such Robust Scaling
df_scaled_1$Balance <- scale(df$Balance, median(df$Balance), scale = IQR(df$Balance))

df_scaled_1$Tenure <- scale(df_scaled_1$Tenure)

df_scaled_1$NumOfProducts <- scale(df_scaled_1$NumOfProducts)

# 'CreditScore' column by the boxplots from the begining of this notebook, seems to be left skewed. So for the same reasons as the 'Balance' column, we will scale the data with the Robust scaler
df_scaled_1$CreditScore <- scale(df$CreditScore, median(df$CreditScore), scale = IQR(df$CreditScore))

head(df_scaled_1)
```

Third Dataframe - balanced

```{r}
df_sampled <- oversampled_data$data
df_sampled_scaled <- df_sampled

# 'Age' variable is right skewed, therefore it may be better to use a scaling method that is less affected by outliers, such as Robust scaling
df_sampled_scaled$Age <- scale(df_sampled$Age, median(df_sampled$Age), scale = IQR(df_sampled$Age))

# 'EstimatedSalary' variable does not contain any outliers, so it would be best to scale it with Standardization scaling that center the values around 0 and scale them to have a standard deviation of 1
df_sampled_scaled$EstimatedSalary <- scale(df_sampled_scaled$EstimatedSalary)

# 'Balance' column shows that 25% of the data lies to the right of the upper quartile value, it suggests that the distribution of the data is skewed to the right. In this case, it may be appropriate to use a scaling method that is robust to skewness, 
# such Robust Scaling
df_sampled_scaled$Balance <- scale(df_sampled$Balance, median(df_sampled$Balance), scale = IQR(df_sampled$Balance))

df_sampled_scaled$Tenure <- scale(df_sampled_scaled$Tenure)

df_sampled_scaled$NumOfProducts <- scale(df_sampled_scaled$NumOfProducts)

# 'CreditScore' column by the boxplots from the begining of this notebook, seems to be left skewed. So for the same reasons as the 'Balance' column, we will scale the data with the Robust scaler
df_sampled_scaled$CreditScore <- scale(df_sampled$CreditScore, median(df_sampled$CreditScore), scale = IQR(df_sampled$CreditScore))

head(df_sampled_scaled)
```

## 3. Classification models




## 3.1.1 Logistic Regression

```{r}
logreg_fit <- glm(Exited ~., data = df)

# 5-fold cross validation to estimate test error (to check if model performs well for the data)
set.seed(1)
cv_error_logreg <- cv.glm(df, logreg_fit, K = 5 ) # k=5 since we have a small number of data (10000)
```

```{r}
# test error of model (average mean-squared error of the 5 folds)
cv_error_logreg$delta[1] # == mean((logreg_probs - df$Exited)^2), delta is in (0,1)

logreg_probs <- predict(logreg_fit, type = "response")
logreg_probs[1:10]

logreg_pred <- rep(0, 10000)
logreg_pred[logreg_probs > .5] = 1

table(logreg_pred, df$Exited)

mean(logreg_pred == df$Exited)
```

Here, the output delta of the function cv.glm represents the mean of squares of the real values (either 0 or 1) and the predicted probabilities for being equal to 1, hence the smaller is the better is.

```{r}
# check if scaling affects the performance of the model
logreg_fit_initial <- glm(Exited ~., data = df_scaled_1)
set.seed(1)
cv_error_logreg_initial <- cv.glm(df, logreg_fit_initial, K = 5 )
cv_error_logreg_initial$delta[1]
logreg_probs_initial <- predict(logreg_fit_initial, type = "response")

logreg_pred_initial <- rep(0, 10000)
logreg_pred_initial[logreg_probs_initial > .5] = 1

table(logreg_pred_initial, df$Exited)

mean(logreg_pred_initial == df$Exited)
```

```{r}
# check if normal scaling affects the performance of the model
logreg_fit_initial <- glm(Exited ~., data = df_scaled)
set.seed(1)
cv_error_logreg_initial <- cv.glm(df, logreg_fit_initial, K = 5 )
cv_error_logreg_initial$delta
logreg_probs_initial <- predict(logreg_fit_initial, type = "response")

logreg_pred_initial <- rep(0, 10000)
logreg_pred_initial[logreg_probs_initial > .5] = 1

table(logreg_pred_initial, df$Exited)

mean(logreg_pred_initial == df$Exited)
```

Logistic Regression seems to perform well considering the accuracy, which is just over 0.8. However we classify points as '0' whereas they are actually '1' (about 0.2), meaning that we predicted customers that they would not exit but they eventually did so. This may happens since we have imbalance in the target column, since we have 7963 customers who stay with the bank and 2037 churned (we have about 80% information for customers who stayed).

```{r}
# investigate the significance of each of the columns through using 5-fold CV
set.seed(1)
train_data <- trainControl(method="cv", number=5) 
model_logreg <- train(as.factor(Exited)~., data = df, family = binomial(), trControl = train_data, method = "glm")
summary(model_logreg)
```

By using 5-fold CV at first glance (assuming that we have all predictors in the model) we can say that CreditScore, Age, Balance, NumOfProducts, IsActiveMember, Germany, and Female are the most important features for classifying the output of variable Exited. Before we proceed just with these predictors we can check how their p-values are affected by dropping just the one with largest p-value each time.

```{r}
# same as before, just without France, due the largest p-value
set.seed(1)
model_logreg1 <- train(as.factor(Exited)~. -France, data = df, family = binomial(), trControl = train_data, method = "glm")
summary(model_logreg1)
```

```{r}
# same as before, just without EstimatedSalary, due the 2nd larger p-value
set.seed(1)
model_logreg2 <- train(as.factor(Exited)~. -EstimatedSalary, data = df, family = binomial(), trControl = train_data, method = "glm")
summary(model_logreg2)
```

```{r}
# explore how model with just all the above significant predictors performs
reduced_model_logreg_fit <- glm(Exited ~ CreditScore+ Age+ Balance+ NumOfProducts+ IsActiveMember+ Germany+ Female, data = df)
set.seed(1)
cv.glm(df, reduced_model_logreg_fit, K = 5 )$delta[1]
reduced_model_logreg_probs <- predict(reduced_model_logreg_fit, type = "response")
reduced_model_logreg_pred <- rep(0, 10000)
reduced_model_logreg_pred[reduced_model_logreg_probs > .5] = 1

table(reduced_model_logreg_pred, df$Exited)

mean(reduced_model_logreg_pred == df$Exited)
```

The reduced model does not make any significance difference, though it is a bit better than the full one. We can investigate more, by using only the 5 most important variables.

```{r}
# explore how model with just the 5 most significant predictors performs 
reduced_model_logreg_fit5 <- glm(Exited ~  Age+ Balance+  IsActiveMember+ Germany+ Female, data = df)
set.seed(1)
cv.glm(df, reduced_model_logreg_fit5, K = 5 )$delta[1]
reduced_model_logreg_probs5 <- predict(reduced_model_logreg_fit5, type = "response")
reduced_model_logreg_pred5 <- rep(0, 10000)
reduced_model_logreg_pred5[reduced_model_logreg_probs5 > .5] = 1

table(reduced_model_logreg_pred5, df$Exited)

mean(reduced_model_logreg_pred5 == df$Exited)
```

As we expected, the last model performs worse than the one which contains all the significant variables even they are less than the others.

Although the accuracy is slightly lower than the full logistic model we have less error (delta) in this case. Thus, this model provides more (negligible though) certainty for the output in comparison to the first one.

As for the specific method, since the accuracies are very closed to each other we could select the last one, due to its simplicity.

```{r}
plot(hatvalues(reduced_model_logreg_fit))
```

```{r}
hats <- as.data.frame(hatvalues(reduced_model_logreg_fit))
hats[order(-hats['hatvalues(reduced_model_logreg_fit)']), ][1:20]
```

From the above plot and first 20 greater leverage values we can see that there are no values with high leverage, hence we do not have to concern about any influence from the outliers, which were detected during the pre-processing.

lasso logistic for selecting the best model for logistic regression

```{r}
x <- model.matrix(Exited ~ ., data=df)[, -9]
y <- df$Exited
lasso.logistic.mod <- glmnet(x, y, alpha = 1, family='binomial')
plot(lasso.logistic.mod)
plot(lasso.logistic.mod, xvar="lambda")
```

```{r}
coef(lasso.logistic.mod)[,10]
```

Lasso regression suggests to keep only Age, IsActiveMember, and Germany.

```{r}
set.seed(1) 
model_logreg <- glm(Exited~ Age+ IsActiveMember+ Germany, data = df)

cv.glm(df, model_logreg, K=5)$delta[1]
logreg_probs_model <- predict(model_logreg, type = "response")

logreg_pred_model <- rep(0, 10000)
logreg_pred_model[logreg_probs_model > .5] = 1

table(logreg_pred_model, df$Exited)

mean(logreg_pred_model == df$Exited)
```

The lasso method suggests a model which provides higher error, as well as lower accuracy, hence we could not select it.

## 3.1.2 Logistic Regression // Oversampled data

```{r}
logreg_fit <- glm(Exited ~., data = df_sampled_scaled)

# 5-fold cross validation to estimate test error (to check if model performs well for the data)
set.seed(1)
cv_error_logreg <- cv.glm(df_sampled_scaled, logreg_fit, K = 5 )
```

```{r}
# test error of model (average mean-squared error of the 5 folds)
cv_error_logreg$delta[1] # == mean((logreg_probs - df$Exited)^2), delta is in (0,1)

logreg_probs <- predict(logreg_fit, type = "response")
logreg_probs[1:10]

logreg_pred <- rep(0, nrow(df_sampled_scaled))
logreg_pred[logreg_probs > .5] = 1

table(logreg_pred, df_sampled_scaled$Exited)

mean(logreg_pred == df_sampled_scaled$Exited)
```

```{r}
# approximate numbers

cat('Sensitivity using the oversampled data:', 5524/(5524+2464))
cat('\nSensitivity using the original data:', 258/(258+1779))
```

The oversampling does not provide better accuracy in logistic model. However, we have better sensitivity here; we are able with this model to predict correctly more customers who will exit the bank. Specifically, we have 0.7 sensitivity whereas by using only initial data we had about 0.13

## 3.2. Naive Bayes

## 3.2.1 Binomial Naives Bayes

```{r}
# create 5-fold cross validation randomly
fold1 <- c(1:8000)
fold2 <- c(1:8000)
fold3 <- c(1:8000)
fold4 <- c(1:8000)
fold5 <- c(1:8000)
train <- data.frame(fold1, fold2, fold3, fold4, fold5)
library(dplyr)
for (i in 1:5){
  set.seed(i)
  train[,i] <- df$Exited %>% createDataPartition(p = 0.8, list = FALSE)
}
```

```{r}
nb_fit1 <- naiveBayes(Exited~ ., data = df, subset = train[,1], type = 'class')
nb_fit2 <- naiveBayes(Exited~ ., data = df, subset = train[,2], type = 'class')
nb_fit3 <- naiveBayes(Exited~ ., data = df, subset = train[,3], type = 'class')
nb_fit4 <- naiveBayes(Exited~ ., data = df, subset = train[,4], type = 'class')
nb_fit5 <- naiveBayes(Exited~ ., data = df, subset = train[,5], type = 'class')
```

```{r}
table(predict(nb_fit1, df[-train[,1],]), df$Exited[-train[,1]])

table(predict(nb_fit2, df[-train[,2],]), df$Exited[-train[,2]])

table(predict(nb_fit3, df[-train[,3],]), df$Exited[-train[,3]])

table(predict(nb_fit4, df[-train[,4],]), df$Exited[-train[,4]])

table(predict(nb_fit5, df[-train[,5],]), df$Exited[-train[,5]])
```

```{r}
accuracy <- c()
for (i in 1:5){
  nb_pred <- predict(naiveBayes(Exited~ ., data = df, subset = train[,i],type = 'class'), df[-train[,i],] )
  accuracy[i] = mean(nb_pred == df$Exited[-train[,i]])
}
accuracy
mean(accuracy)
```

We can see that the Naive Bayes model performs better than both reduced and full Logistic regression model. We can investigate if we have better accuracy in the case of less predictors.

```{r}
# Age+ Balance+  IsActiveMember+ Germany+ Female
accuracy <- c()
for (i in 1:5){
  nb_pred <- predict(naiveBayes(Exited~ Age+ Balance+  IsActiveMember+ Germany+ Female, data = df, subset = train[,i], type = 'class'), df[-train[,i],] )
  accuracy[i] = mean(nb_pred == df$Exited[-train[,i]])
}
accuracy
mean(accuracy)
```

The reduced Naive Bayes model does not perform better that the full one.

## 3.2.2 Naive Bayes with kernel density estimator

```{r}
y <- as.character(df$Exited)
accuracy <- c()
for (i in 1:5){
  nb_pred <- predict(naive_bayes(y~ ., data = df, subset = train[,i], usekernel=T), df[-train[,i],] )
  accuracy[i] = mean(nb_pred == y[-train[,i]])
}
accuracy
mean(accuracy)
```

```{r}
nb_fit1 <- naive_bayes(y~ ., data = df, subset = train[,1],usekernel=T)
nb_fit2 <- naive_bayes(y~ ., data = df, subset = train[,2],usekernel=T)
nb_fit3 <- naive_bayes(y~ ., data = df, subset = train[,3],usekernel=T)
nb_fit4 <- naive_bayes(y~ ., data = df, subset = train[,4],usekernel=T)
nb_fit5 <- naive_bayes(y~ ., data = df, subset = train[,5],usekernel=T)

# tables for 5 cases
table(predict(nb_fit1, df[-train[,1],]), y[-train[,1]])

table(predict(nb_fit2, df[-train[,2],]), y[-train[,2]])

table(predict(nb_fit3, df[-train[,3],]), y[-train[,3]])

table(predict(nb_fit4, df[-train[,4],]), y[-train[,4]])

table(predict(nb_fit5, df[-train[,5],]), y[-train[,5]])
```

```{r}
# use all data
nb_fit1 <- naive_bayes(y~ ., data = df,usekernel=T)
table(predict(nb_fit1, df), y)
```


We can see that the accuracy of the Naive Bayes algorithm with all predictors is extremely nice for our data just under 0.97, as well as the sensitivity which is about 0.85.

We can investigate to improve more the sensitivity by using the oversampled data.

## 3.2.3 Naive Bayes with Oversampled Data

```{r}
# using validation approach
set.seed(1)
train <- df_sampled_scaled$Exited %>% createDataPartition(p = 0.8, list = FALSE)
```

```{r}
nb_fit1 <- naiveBayes(Exited~ ., data = df_sampled_scaled, subset = train,type = 'class')
table(predict(nb_fit1, df_sampled_scaled[-train,]), df_sampled_scaled$Exited[-train])
nb_pred <- predict(naiveBayes(Exited~ ., data = df_sampled_scaled, subset = train,type = 'class'), df_sampled_scaled[-train,] )
accuracy <- mean(nb_pred == df_sampled_scaled$Exited[-train])
cat('Accuracy using the oversample data:',accuracy)
```

```{r}
# approximate numbers

cat('Sensitivity using the oversample data:',1089/(1089+501))
```

```{r}
# Naive Bayes with kernel density estimator
set.seed(1)
y <- as.character(df_sampled_scaled$Exited)
nb_fit_kernel <- naive_bayes(y~ ., data = df_sampled_scaled, subset = train, usekernel=T)
table(predict(nb_fit_kernel, df_sampled_scaled[-train,]), y[-train])
```

Naive Bayes by using a kernel density estimator for the densities of fk(xj) on the oversampled data provides perfect accuracy and sensitivity; only very few customers of the test set predicts incorrectly.

```{r}
# training confusion matrix
nb_fit_kernel <- naive_bayes(y~ ., data = df_sampled_scaled, usekernel=T)
table(predict(nb_fit_kernel, df_sampled_scaled), y)
```

## 3.3. QDA

```{r}
# Create separate data frames for churned and not churned customers
df_churned <- df[df$Exited == 1, ]
df_not_churned <- df[df$Exited == 0, ]

# Convert the data frame from wide format to long format using gather
df_long <- gather(df, key = "variable", value = "value", -Exited)
df_long$Exited <- factor(df_long$Exited)
# Plot histograms of each variable separated by churn and not churned customers using facet_wrap
ggplot(df_long, aes(value, fill = Exited)) + 
  geom_histogram(bins = 30, alpha = 0.5, position = "identity") +
  facet_wrap(~ variable, scales = "free") +
  labs(x = "", y = "Count", fill = "Exited")
```

## 3.3.1. QDA simple fit with all the data

```{r}
qda.fit <- qda(Exited ~., data = df_scaled_1)
qda.fit
```

```{r}
# Make predictions on the training data
set.seed(1234)
train_pred <- predict(qda.fit, df_scaled_1)

# Calculate accuracy
train_accuracy <- mean(train_pred$class == df_scaled_1$Exited)
train_accuracy 
```

The accuracy is pretty good, although it is on the training data. So, we need to seperate our data into train and test, in order to figure out how our model can perform with unseen data, with cross validation.

## 3.3.2. QDA Cross Validation, with threshold 0.5

```{r}
set.seed(1234)
X <- df_scaled_1[ , -which(names(df_scaled_1) == "Exited")] # Predictor variables
y <- as.factor(df_scaled_1$Exited)# Response variable

model <- train(x=X, y=y, method="qda")# Define qda

k <- 5 # Define the number of splits for k-fold cross-validation
folds <- createFolds(y, k=k)

conf_matrices <- list() # store confusion matrices
predicted_all <- c() # store predicted y's of test set, of all folds 
actual_all <- c() # store their actual y's of test set, of all folds

# Use cross-validation to evaluate qda's performance and calculate confusion matrices, with the help of the created lists
for(i in 1:k){
  test_idx <- folds[[i]] 
  train_idx <- setdiff(1:length(y), test_idx)
  model <- train(x=X[train_idx, ], y=y[train_idx], method="qda")
  predicted <- predict(model, newdata=X[test_idx, ])
  conf_matrix <- table(predicted, y[test_idx])
  conf_matrices[[i]] <- conf_matrix
  
  # Store predicted and actual values for this fold
  predicted_all <- c(predicted_all, predicted)
  actual_all <- c(actual_all, y[test_idx])
  
  # Print the confusion matrix for this fold
  cat("Confusion matrix for fold ", i, ":\n")
  print(conf_matrix)
  
  # Print the number of observations in test set for this fold
  cat("Number of observations in test set for fold ", i, ":", length(test_idx), "\n")
}

accuracy <- mean(predicted_all == actual_all) # Calculate the overall accuracy

cat("Cross-validation accuracy:", round(accuracy, 3), "\n")
```

```{r}
# RECALL and PRECISION for all folds:
TP <- 153
FN <- 255
FP <- 78
RECALL <- TP / (TP + FN)
PRECISION <- TP / (TP + FP)

TP1 <- 165
FN1 <- 242
FP1 <- 85
RECALL1 <- TP1 / (TP1 + FN1)
PRECISION1 <- TP1 / (TP1 + FP1)

TP2 <- 159
FN2 <- 248
FP2 <- 78
RECALL2 <- TP2 / (TP2 + FN2)
PRECISION2 <- TP2 / (TP2 + FP2)

TP3 <- 155
FN3 <- 252
FP3 <- 93
RECALL3 <- TP3 / (TP3 + FN3)
PRECISION3 <- TP3 / (TP3 + FP3)

TP4 <- 155
FN4 <- 252
FP4 <- 93
RECALL4 <- TP4 / (TP4 + FN4)
PRECISION4 <- TP4 / (TP4 + FP4)

cat("Average Recall:", (RECALL + RECALL1 + RECALL2 + RECALL3 + RECALL4)/5)
cat("\nAverage Precision:", (PRECISION + PRECISION1 + PRECISION2 + PRECISION3 + PRECISION4)/5)
# We can clearly see that recall rate is low
```

We can clearly see that the average recall rate is pretty low, even though, we must be more consetrated in maximizing the recall rate, and not precision.

## 3.3.3. QDA - ROC curve

```{r}
set.seed(1234)
# Define response variable
y <- as.factor(df_scaled_1[, 9])

# Perform 5-fold cross-validation
cv <- createFolds(y, k = 5)

# Initialize vector to store predicted probabilities
probs <- rep(0, nrow(df_scaled_1))

# Loop through each fold of the cross-validation
for (i in 1:5) {
  # Get indices for training and testing data
  train_indices <- unlist(cv[-i])
  test_indices <- cv[[i]]
  
  # Fit QDA model to training data
  qda_model <- qda(df_scaled_1[train_indices, -9], y[train_indices])
  
  # Predict class probabilities for testing data
  probs[test_indices] <- predict(qda_model, df_scaled_1[test_indices, -9])$posterior[, 2]
}

# Generate ROC curve and calculate AUC
roc <- roc(y, probs)
auc <- auc(roc)

# Plot ROC curve with thresholds
plot(roc, main = paste("ROC Curve (AUC =", round(auc, 2), ")"), print.thres = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1), print.auc = TRUE)
```

In k-fold cross-validation, the data is divided into k folds, and each fold is used as a validation set while the rest of the data is used for training. This process is repeated k times, with each fold used exactly once as a validation set.

At the end of k-fold cross-validation, you have k different models (one for each fold), and you can calculate the performance metric (such as ROC-AUC) for each of them. The overall performance metric is then typically calculated as the average of the performance metrics across all folds. As we can see the model performs better than a random model.

Each point out of ten, represent a threshold. As the threshold decreases, the Specificity decreases and the Sensitivity increases. For example, for the threshold 0.4, the True Positive rate, which is the Recall rate is at 0.474, while the False positive rate is at 0.916. When the threshold is set to 0.2, the recall rate increases form 0.474 to 0.720 and the specificity decreases from 0.916 to 0.749. It is not bad at all! When it comes to specificity, the model, even after lowering the threshold, still predicted correctly as negative a lot of cases, without marking too many cases as false positive! That are some good news, we could lower the threshold, get better recall, and and get not that too bad of a precision rate!

Let's set threshold lower than 0.5, as we are interested to predict correctly the clients who will churn. By lowering the threshold, we will achieve a higher Recall rate, as it will accept as a positive instance easier than before.

## 3.3.4. QDA - QDA Cross Validation, with threshold 0.2

```{r}
set.seed(1234)
X <- df_scaled_1[ , -which(names(df_scaled_1) == "Exited")] # Predictor variables
y <- as.factor(df_scaled_1$Exited)# Response variable

model <- train(x=X, y=y, method="qda")# Define qda

k <- 5 # Define the number of splits for k-fold cross-validation
folds <- createFolds(y, k=k)

conf_matrices <- list() # store confusion matrices
predicted_all <- c() # store predicted y's of test set, of all folds 
actual_all <- c() # store their actual y's of test set, of all folds

# Use cross-validation to evaluate qda's performance and calculate confusion matrices, with the help of the created lists
for(i in 1:k){
  test_idx <- folds[[i]] 
  train_idx <- setdiff(1:length(y), test_idx)
  model <- train(x=X[train_idx, ], y=y[train_idx], method="qda")
  predicted_probs <- predict(model, newdata = X[test_idx, ], type = "prob")[, 2]
  predicted_labels <- ifelse(predicted_probs >= 0.2, "1", "0") # Set threshold to 0.2
  conf_matrix <- table(predicted = predicted_labels, actual = y[test_idx])
  conf_matrices[[i]] <- conf_matrix
  
  # Store predicted and actual values for this fold
  predicted_all <- c(predicted_all, predicted)
  actual_all <- c(actual_all, y[test_idx])
  
  # Print the confusion matrix for this fold
  cat("Confusion matrix for fold ", i, ":\n")
  print(conf_matrix)
  
  # Print the number of observations in test set for this fold
  cat("Number of observations in test set for fold ", i, ":", length(test_idx), "\n")
}

accuracy <- mean(predicted_all == actual_all) # Calculate the overall accuracy

cat("Cross-validation accuracy:", round(accuracy, 3), "\n")
```

```{r}
# RECALL AND PRECISION for the first fold
TP <- 302
FN <- 105
FP <- 428
RECALL <- TP / (TP + FN)
PRECISION <- TP / (TP + FP)

TP1 <- 283
FN1 <- 125
FP1 <- 408
RECALL1 <- TP1 / (TP1 + FN1)
PRECISION1 <- TP1 / (TP1 + FP1)

TP2 <- 282
FN2 <- 125
FP2 <- 375
RECALL2 <- TP2 / (TP2 + FN2)
PRECISION2 <- TP2 / (TP2 + FP2)

TP3 <- 302
FN3 <- 105
FP3 <- 402
RECALL3 <- TP3 / (TP3 + FN3)
PRECISION3 <- TP3 / (TP3 + FP3)

TP4 <- 287
FN4 <- 121
FP4 <- 397
RECALL4 <- TP4 / (TP4 + FN4)
PRECISION4 <- TP4 / (TP4 + FP4)


cat("Average Recall:", (RECALL + RECALL1 + RECALL2 + RECALL3 + RECALL4)/5)
cat("\nAverage Precision:", (PRECISION + PRECISION1 + PRECISION2 + PRECISION3 + PRECISION4)/5)
# we can see that now, the recall rate got better by just lowering the threshold!
```

Lowering the threshold will generally increase the recall rate, which means that the model will correctly identify more positive cases. This is because the lower threshold will classify more instances as positive, including some that were previously classified as negative, thus reducing the number of false negatives. However, this also means that the model is likely to have a higher false positive rate, which means that it may incorrectly classify some negative cases as positive.

## 3.3.5. QDA - QDA Cross Validation, with threshold 0.2 - Oversampled data

```{r}
set.seed(1234)
X <- df_sampled_scaled[ , -which(names(df_sampled_scaled) == "Exited")] # Predictor variables
y <- as.factor(df_sampled_scaled$Exited)# Response variable

model <- train(x=X, y=y, method="qda")# Define qda

k <- 5 # Define the number of splits for k-fold cross-validation
folds <- createFolds(y, k=k)

conf_matrices <- list() # store confusion matrices
predicted_all <- c() # store predicted y's of test set, of all folds 
actual_all <- c() # store their actual y's of test set, of all folds

# Use cross-validation to evaluate qda's performance and calculate confusion matrices, with the help of the created lists
for(i in 1:k){
  test_idx <- folds[[i]] 
  train_idx <- setdiff(1:length(y), test_idx)
  model <- train(x=X[train_idx, ], y=y[train_idx], method="qda")
  predicted_probs <- predict(model, newdata = X[test_idx, ], type = "prob")[, 2]
  predicted_labels <- ifelse(predicted_probs >= 0.2, "1", "0") # Set threshold to 0.2
  conf_matrix <- table(predicted = predicted_labels, actual = y[test_idx])
  conf_matrices[[i]] <- conf_matrix
  
  # Store predicted and actual values for this fold
  predicted_all <- c(predicted_all, predicted)
  actual_all <- c(actual_all, y[test_idx])
  
  # Print the confusion matrix for this fold
  cat("Confusion matrix for fold ", i, ":\n")
  print(conf_matrix)
  
  # Print the number of observations in test set for this fold
  cat("Number of observations in test set for fold ", i, ":", length(test_idx), "\n")
}

accuracy <- mean(predicted_all == actual_all) # Calculate the overall accuracy

cat("Cross-validation accuracy:", round(accuracy, 3), "\n")
```

```{r}
TP <-1517
FP <- 979
FN <- 83
RECALL <- TP/(TP + FN)
PRECISION <- TP/(TP + FP)

TP1 <-1516
FP1 <- 985
FN1 <- 84
RECALL1 <- TP1/(TP1 + FN1)
PRECISION1 <- TP1/(TP1 + FP1)

TP2 <-1491
FP2 <- 957
FN2 <- 109
RECALL2 <- TP2/(TP2 + FN2)
PRECISION2 <- TP2/(TP2 + FP2)

TP3 <-1506
FP3 <- 977
FN3 <- 93
RECALL3 <- TP3/(TP3 + FN3)
PRECISION3 <- TP3/(TP3 + FP3)

TP4 <-1497
FP4 <- 976
FN4 <- 103
RECALL4 <- TP4/(TP4 + FN4)
PRECISION4 <- TP4/(TP4 + FP4)

cat("Average Recall:", (RECALL + RECALL1 + RECALL2 + RECALL3 + RECALL4)/5)
cat("\nAverage Precision:", (PRECISION + PRECISION1 + PRECISION2 + PRECISION3 + PRECISION4)/5)
```

After performing the quadradic model with all the variables, as well as on a subset of variables (we thought were the best ones), it is correct to use the technique PCA, where it reduces the problem of multicollinearity (the coefficients that the qda model will not be reliable due to the inflation of the standard errors), by combining explanatory variables to a smaller set of uncorrelated variables. This method will also give us the necessary information of each variable, so we use all the resources we have!

## 3.3.6. PCA

```{r}
set.seed(1234)

predictors <- as.matrix(df_scaled_1[, -9])
pca <- prcomp(predictors, scaled = FALSE)
loadings <- pca$rotation
loadings
cor(pca$x)
summary(pca)
#qda_model <- qda(pca$x, df_scaled_1[, 9])
```

From the first output we can say that, when we take a specific observasion, and multiply for each feature the corresponding value that the first PCA column gives, and at the end add them all, we will get the principal componet score for that specific observation.

Each row corresponds to a variable, and each column corresponds to a PC. (when priting the loadings)

We can see now that the problem of multicollinearity is resolved. The values in each cell represent the loading of that variable on that PC, which can be interpreted as the correlation between the variable and the PC.

For example, the first variable "CreditScore" has a loading of 0.013 on PC1, indicating that it is weakly positively correlated with the first principal component. However, it has a loading of -0.998 on PC5, indicating that it is strongly negatively correlated with the fifth principal component.

Similarly, the second variable "Age" has a loading of -0.110 on PC1, indicating that it is weakly negatively correlated with the first principal component, but it has a loading of -0.989 on PC4, indicating that it is strongly negatively correlated with the fourth principal component.

Lastly, when we print the correlation of the pc's we can see that multicollinearity is resolved.

```{r}
# Create the plot
plot(pca, type = "l")
biplot(pca)
```

As we can see, the most important variables, that explain most of the variance, are EstimatedSalary, Tenure, and NumOfProducts. Variables with higher values on PC1 are positively correlated with higher tenure and estimated salary, but not as strongly correlated with NumOfProducts. Similarly, variables with higher values on PC2 are negatively correlated with estimated salary and tenure, but positively correlated with NumOfProducts.

```{r}
df_scaled_1_pca <- cbind(df_scaled_1, pca$x[, 1:6])

df_scaled_1_pca$Exited <- as.factor(df_scaled_1_pca$Exited)

df_scaled_1_pca <- na.omit(df_scaled_1_pca)

library(ggplot2)

ggplot(df_scaled_1_pca, aes(x = PC1, y = PC2, color = factor(Exited))) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "PCA Plot",
       x = "PC1", y = "PC2", color = "Exited")

# while clustering the two groups, it can be clearly seen that clients who churned have a high value of pc1 and pc2
```

Overall, it seems that just the first 6 principal components would explain a total of 83.2% of the variability. (The PC1 explains 18.42% of the variance and PC2 explains 17.60% of the variance and so on). We select as best 6 PC's as common rule of thumb is to retain enough components to explain at least 70-80% of the total variability in the predictor variables

## 3.3.6.1 QDA - PCA with threshold 0.5

```{r}
# Split data into train and test sets
set.seed(123)
df_pca <- df_scaled_1_pca[, c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "Exited")]
train_idx <- sample(nrow(df_pca), round(0.8 * nrow(df_pca)))
train_data <- df_pca[train_idx, ]
test_data <- df_pca[-train_idx, ]

# Define training control
ctrl <- trainControl(method = "cv", number = 5)

# Fit QDA model with cross-validation
qda_fit <- train(Exited ~ ., data = train_data, method = "qda", trControl = ctrl)

# Make predictions on test data
pred <- predict(qda_fit, newdata = test_data)

conf_mat <- confusionMatrix(pred, test_data$Exited)
conf_mat

TP <- conf_mat$table[2, 2]
FN <- conf_mat$table[1, 2]
FP <- conf_mat$table[2, 1]
RECALL <- TP / (TP + FN)
PRECISION <- TP / (TP + FP)
cat("RECALL: ", round(RECALL, 2), "\nPRECISION: ", round(PRECISION, 2))
```

## 3.3.6.2 QDA - PCA with threshold 0.2

```{r}
# Split data into train and test sets
set.seed(123)
df_pca <- df_scaled_1_pca[, c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "Exited")]
train_idx <- sample(nrow(df_pca), round(0.8 * nrow(df_pca)))
train_data <- df_pca[train_idx, ]
test_data <- df_pca[-train_idx, ]

# Define training control
ctrl <- trainControl(method = "cv", number = 5)

# Fit QDA model with cross-validation
qda_fit <- train(Exited ~ ., data = train_data, method = "qda", trControl = ctrl)

# Make predictions on test data with a threshold of 0.2
pred_probs <- predict(qda_fit, newdata = test_data, type = "prob")
pred_labels <- ifelse(pred_probs[, "1"] >= 0.2, "1", "0")
pred_labels <- factor(pred_labels, levels = c("0", "1"))
test_labels <- factor(test_data$Exited, levels = c("0", "1"))
conf_mat <- confusionMatrix(pred_labels, test_labels)

# Calculate recall and precision
conf_mat
TP <- conf_mat$table[2, 2]
FN <- conf_mat$table[1, 2]
FP <- conf_mat$table[2, 1]
RECALL <- TP / (TP + FN)
PRECISION <- TP / (TP + FP)
cat("RECALL: ", round(RECALL, 2), "\nPRECISION: ", round(PRECISION, 2))
```

When the threshold is set to 0.2, we get better recall results!

## 3.4. KNN

## 3.4.1 KNN, Split into Train and Test, Evaluate Performance

Split the data into train.X train.y test.X and test.y

```{r}
X <- df_scaled_1[, -9]
y <- df_scaled_1[, 9]

set.seed(42)# Set the random seed for reproducibility

train_idx <- createDataPartition(y, p = 0.8, list = FALSE)
train.X <- X[train_idx, ]
train.y <- y[train_idx]
test.X <- X[-train_idx, ]
test.y <- y[-train_idx]
train_subset <- cbind(train.X, train.y)
test_subset <- cbind(test.X, test.y)
```

```{r}
set.seed(1234)
knn.pred <- knn(train.X, test.X, train.y, k = 3)
table(knn.pred, test.y)
```

## 3.4.2 Cross Validation on Searching the Best k that Maximizes the Recall Rate 

As our goal is to maximize the recall rate, we perform a cros validation of 5, for each k that ranges from 1 to 20, and calculate the average recall rate for each. Then the output refers to the k that maximizes the Recall rate

```{r}
set.seed(1234)

X <- df_scaled_1[, -which(names(df_scaled_1) == "Exited")] # Predictor variables
y <- as.factor(df_scaled_1$Exited) # Response variable

# Define the range of k values to test
k_range <- 1:20

# Define the number of splits for k-fold cross-validation
k_fold <- 5

# Create a list to store cross-validation results
cv_results <- list()

# Perform k-fold cross-validation for each k value in the range
for (k in k_range) {
  # Define the k-fold cross-validation procedure
  folds <- createFolds(y, k = k_fold)

  # Create a list to store recall scores for each fold
  recall_scores <- c()

  # Iterate over the folds and calculate the recall score for each one
  for (i in 1:k_fold) {
    # Split the data into training and test sets for this fold
    test_idx <- folds[[i]]
    train_idx <- setdiff(1:length(y), test_idx)
    X_train <- X[train_idx, ]
    y_train <- y[train_idx]
    X_test <- X[test_idx, ]
    y_test <- y[test_idx]

    # Train a KNN model with the current k value using the training set
    model <- train(X_train, y_train, method = "knn", trControl = trainControl(method = "none"), tuneGrid = data.frame(k = k))

    # Make predictions on the test set and calculate recall score
    pred <- predict(model, X_test)
    cm <- confusionMatrix(pred, y_test, positive = "1")
    recall_score <- cm$byClass[["Recall"]]
    recall_scores <- c(recall_scores, recall_score)
  }

  # Store the average recall score for this k value
  cv_results[[k]] <- mean(recall_scores)
}

# Find the k value with the highest average recall score
optimal_k <- k_range[which.max(unlist(cv_results))]

cat("Optimal k value:", optimal_k, "\n")
cv_results
```

The reason the cross validation gives as the best k to be equal to 1, is that because the minority of class is the positive instances, clients who churned. This is because with a lower value of k, the algorithm is more likely to select neighbors that belong to the positive class, which can improve the recall for the positive class. Although, we will choose to select the k to be equal to 2, so that we do not overfit the model.

We figured that it would be a waste of time to try and fit a KNN model, at first on the whole set, after divide in train and test, and etc. So, we decided to perform KNN, with cross validation equal to 5, and test the accuracy of the 5 models on different subsets, so called validation sets. Before moving into the procedure as explained above, it is a must to scale the data first. Variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. It is good to mention that we use K equal to 2(each new observation, the algorithm will identify the 2 nearest neighbors (in terms of distance) from the training set and assign the class that is most common among those 2 neighbor to the new observation.)

## 3.4.3. KNN -  Cross Validation with k = 2

```{r}
set.seed(1234)
X <- df_scaled_1[ , -which(names(df_scaled_1) == "Exited")] # Predictor variables
y <- as.factor(df_scaled_1$Exited)# Response variable

k <- 5 # Define the number of splits for k-fold cross-validation
folds <- createFolds(y, k=k)

conf_matrices <- list() # store confusion matrices
predicted_all <- c() # store predicted y's of test set, of all folds 
actual_all <- c() # store their actual y's of test set, of all folds

# Use cross-validation to evaluate k-NN's performance and calculate confusion matrices, with the help of the created lists
for(i in 1:k){
  test_idx <- folds[[i]] 
  train_idx <- setdiff(1:length(y), test_idx)
  
  # Fit k-NN model using the training set, with k=2
  model <- train(x=X[train_idx, ], y=y[train_idx], method="knn", trControl=trainControl(method="cv", number=5), tuneGrid = data.frame(k = 2))
  
  # Make predictions on the test set
  predicted <- predict(model, newdata=X[test_idx, ])
  
  # Calculate confusion matrix for this fold and store it in the list
  conf_matrix <- table(predicted, y[test_idx])
  conf_matrices[[i]] <- conf_matrix
  
  # Store predicted and actual values for this fold
  predicted_all <- c(predicted_all, predicted)
  actual_all <- c(actual_all, y[test_idx])
  
  # Print the confusion matrix for this fold
  cat("Confusion matrix for fold ", i, ":\n")
  print(conf_matrix)
  
  # Print the number of observations in test set for this fold
  cat("Number of observations in test set for fold ", i, ":", length(test_idx), "\n")
}

accuracy <- mean(predicted_all == actual_all) # Calculate the overall accuracy

cat("Cross-validation accuracy:", round(accuracy, 3), "\n")
```

```{r}
TP <- 185
FP <- 222
FN <- 223
RECALL <- TP/(TP+FN)
PRECISION <- TP/(TP+FP)

TP1 <- 200
FP1 <- 194
FN1 <- 207
RECALL1 <- TP1/(TP1+FN1)
PRECISION1 <- TP1/(TP1+FP1)

TP2 <- 206
FP2 <- 194
FN2 <- 201
RECALL2 <- TP2/(TP2+FN2)
PRECISION2 <- TP2/(TP2+FP2)

TP3 <- 185
FP3 <- 198
FN3 <- 223
RECALL3 <- TP3/(TP3+FN3)
PRECISION3 <- TP3/(TP3+FP3)

TP4 <- 178
FP4 <- 192
FN4 <- 229
RECALL4 <- TP4/(TP4+FN4)
PRECISION4 <- TP4/(TP4+FP4)

cat("Average Recall:", (RECALL + RECALL1 + RECALL2 + RECALL3 + RECALL4)/5)
cat("\nAverage Precision:", (PRECISION + PRECISION1 + PRECISION2 + PRECISION3 + PRECISION4)/5)
```

## 3.4.4. KNN -  Cross Validation with k = 2, PCA

```{r}
set.seed(123)
df_pca <- df_scaled_1_pca[, c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "Exited")]
train_idx <- sample(nrow(df_pca), round(0.8 * nrow(df_pca)))
train_data <- df_pca[train_idx, ]
test_data <- df_pca[-train_idx, ]

# Define training control
ctrl <- trainControl(method = "cv", number = 5)

# Fit knn model with 1 neighbor and cross-validation
knn_fit <- train(Exited ~ ., data = train_data, method = "knn", trControl = ctrl, tuneGrid = data.frame(k = 2))

# Make predictions on test data
pred <- predict(knn_fit, newdata = test_data)

conf_mat <- confusionMatrix(pred, test_data$Exited)
conf_mat

TP <- conf_mat$table[2, 2]
FN <- conf_mat$table[1, 2]
FP <- conf_mat$table[2, 1]
RECALL <- TP / (TP + FN)
PRECISION <- TP / (TP + FP)
cat("RECALL: ", round(RECALL, 2), "\nPRECISION: ", round(PRECISION, 2))
```

## 3.4.4. KNN -  Cross Validation with k = 2, Oversampled Data

```{r}
set.seed(1234)
X <- df_sampled_scaled[ , -which(names(df_sampled_scaled) == "Exited")] # Predictor variables
y <- as.factor(df_sampled_scaled$Exited)# Response variable

k <- 5 # Define the number of splits for k-fold cross-validation
folds <- createFolds(y, k=k)

conf_matrices <- list() # store confusion matrices
predicted_all <- c() # store predicted y's of test set, of all folds 
actual_all <- c() # store their actual y's of test set, of all folds

# Use cross-validation to evaluate k-NN's performance and calculate confusion matrices, with the help of the created lists
for(i in 1:k){
  test_idx <- folds[[i]] 
  train_idx <- setdiff(1:length(y), test_idx)
  
  # Fit k-NN model using the training set, with k=2
  model <- train(x=X[train_idx, ], y=y[train_idx], method="knn", trControl=trainControl(method="cv", number=5), tuneGrid = data.frame(k = 2))
  
  # Make predictions on the test set
  predicted <- predict(model, newdata=X[test_idx, ])
  
  # Calculate confusion matrix for this fold and store it in the list
  conf_matrix <- table(predicted, y[test_idx])
  conf_matrices[[i]] <- conf_matrix
  
  # Store predicted and actual values for this fold
  predicted_all <- c(predicted_all, predicted)
  actual_all <- c(actual_all, y[test_idx])
  
  # Print the confusion matrix for this fold
  cat("Confusion matrix for fold ", i, ":\n")
  print(conf_matrix)
  
  # Print the number of observations in test set for this fold
  cat("Number of observations in test set for fold ", i, ":", length(test_idx), "\n")
}

accuracy <- mean(predicted_all == actual_all) # Calculate the overall accuracy

cat("Cross-validation accuracy:", round(accuracy, 3), "\n")
```

```{r}
TP <- 1536
FP <- 370
FN <- 64
RECALL <- TP/(TP+FN)
PRECISION <- TP/(TP+FP)

TP1 <- 1524
FP1 <- 333
FN1 <- 75
RECALL1 <- TP1/(TP1+FN1)
PRECISION1 <- TP1/(TP1+FP1)

TP2 <- 1521
FP2 <- 347
FN2 <- 79
RECALL2 <- TP2/(TP2+FN2)
PRECISION2 <- TP2/(TP2+FP2)

TP3 <- 1524
FP3 <- 377
FN3 <- 76
RECALL3 <- TP3/(TP3+FN3)
PRECISION3 <- TP3/(TP3+FP3)

TP4 <- 1522
FP4 <- 349
FN4 <- 78
RECALL4 <- TP4/(TP4+FN4)
PRECISION4 <- TP4/(TP4+FP4)

cat("Average Recall:", (RECALL + RECALL1 + RECALL2 + RECALL3 + RECALL4)/5)
cat("\nAverage Precision:", (PRECISION + PRECISION1 + PRECISION2 + PRECISION3 + PRECISION4)/5)
```

## 3.5. LDA

#Regular Dataset

```{r}
#Runing on the regular datasetinstall.packages('caret')
library('caret')
trainIndex <- createDataPartition(df$Exited, p = 0.8, list = FALSE)

# Split the data into training and test sets (80% training, 20% test)
train_data_subset <- df[trainIndex, ]
test_data_subset <- df[-trainIndex, ]
```

```{r}
#Simple LDA
library("MASS")
linear <- lda(Exited~., train_data_subset)
linear
```

```{r}
p2 <- predict(linear, test_data_subset)$class
tab1 <- table(Predicted = p2, Actual = test_data_subset$Exited)
tab1
Accuracy <- sum(diag(tab1))/sum(tab1)
Accuracy
```

```{r}
par(mar = c(4, 4, 2, 2))
plot(linear)
```

Oversampled

```{r}
trainIndex <- createDataPartition(oversampled_churn_data$Exited, p = 0.8, list = FALSE)

# Split the data into training and test sets (80% training, 20% test)
train_data_subset <- df[trainIndex, ]
test_data_subset <- df[-trainIndex, ]
```

```{r}
#Simple LDA
linear <- lda(Exited~., train_data_subset)
linear
```
**If** you have only LD1, it means that there is only one significant discriminant function that separates the classes well. This could be due to various reasons such as low sample size, low variability in the predictor variables, or a high degree of overlap between the classes. We invesitage further

MOdelling with 5 Folds CV/Scaling/Centering

Oversampled Data

```{r}
set.seed(1)
#Set Level using code below (incase it returns error "type is regression")
oversampled_churn_data$Exited <- factor(oversampled_churn_data$Exited, levels = c("0", "1"))
levels(oversampled_churn_data$Exited) <- make.names(levels(oversampled_churn_data$Exited))

# Define the independent variables
predictors <- c("CreditScore","Age","Tenure","Balance","NumOfProducts","HasCrCard","IsActiveMember","EstimatedSalary","France","Germany","Female")

# Define the target variable
target <- "Exited"

# Define the number of folds for cross-validation
k <- 5

#Making Test Data
test_index <- createDataPartition(oversampled_churn_data$Exited, p = 0.2, list = FALSE)
test_data <- oversampled_churn_data[test_index, ]
train_data <- oversampled_churn_data[-test_index, ]

# Create the training control object
train_control <- trainControl(method = "cv", number = k)

# Train the LDA model with cross-validation
lda_model <- train(as.formula(paste(target, "~", paste(predictors, collapse = "+"))),
                   data = train_data,
                   preProcess = c("center", "scale"),
                   method = "lda",
                   trControl = train_control)

# Print the model results
print(lda_model)

# Make predictions on the test set
predictions <- predict(lda_model, newdata = test_data, type="raw")


# Calculate the accuracy
accuracy <- confusionMatrix(predictions, test_data$Exited)
print(accuracy)
#print(paste("Accuracy:", accuracy))
```
```{r}
#Plot ROC
predictions <- predict(lda_model, newdata = test_data, type="prob")
library(pROC)
roc_obj <- roc(test_data$Exited, predictions$X1)

# Get the optimal operating point
opt_point <- coords(roc_obj, "best", ret=c("threshold", "specificity", "sensitivity"))


# Plot the ROC curve with the optimal point
plot(roc_obj, col = "darkblue", main = "ROC Curve", print.thres = c(0.1, 0.2, 0.3, 0.4, 0.5,0.6,0.7,0.8,0.9), print.auc = TRUE)
points(opt_point$specificity, opt_point$sensitivity, pch = 19, col = "red")
text(opt_point$specificity , opt_point$sensitivity ,
     paste0("Threshold = ", round(opt_point$threshold, 2),
            "\nSensitivity = ", round(opt_point$sensitivity, 2),
            "\nSpecificity = ", round(opt_point$specificity, 2)),
     pos = 2)
```
Regular Df

```{r}
#Set Level

df$Exited <- factor(df$Exited, levels = c("0", "1"))
levels(df$Exited) <- make.names(levels(df$Exited))

# Define the independent variables
predictors <- c("CreditScore","Age","Tenure","Balance","NumOfProducts","HasCrCard","IsActiveMember","EstimatedSalary","France","Germany","Female")

# Define the target variable
target <- "Exited"

# Define the number of folds for cross-validation
k <- 5

#Making Test Data
test_index <- createDataPartition(df$Exited, p = 0.3, list = FALSE)
test_data <- df[test_index, ]
train_data <- df[-test_index,]


# Create the training control object
train_control <- trainControl(method = "cv", number = k)

# Train the LDA model with cross-validation
lda_model <- train(as.formula(paste(target, "~", paste(predictors, collapse = "+"))),
                   data = train_data,
                   preProcess = c("center", "scale"),
                   method = "lda",
                   trControl = train_control)

# Print the model results
print(lda_model)

# Make predictions on the test set
predictions <- predict(lda_model, newdata = test_data, type = "raw")


# Calculate the accuracy
accuracy <- confusionMatrix(predictions, test_data$Exited)
print(accuracy)
#print(paste("Accuracy:", accuracy))
```

```{r}

predictions <- predict(lda_model, newdata = test_data, type = "prob")

# Calculate ROC curve and AUC
# Calculate the ROC curve
library(pROC)
roc_obj <- roc(test_data$Exited, predictions$X1)

# Get the optimal operating point
opt_point <- coords(roc_obj, "best", ret=c("threshold", "specificity", "sensitivity"))


# Plot the ROC curve with the optimal point
plot(roc_obj, col = "darkblue", main = "ROC Curve", print.thres = c(0.1, 0.2, 0.3, 0.4, 0.5,0.6,0.7,0.8,0.9), print.auc = TRUE)
points(opt_point$specificity, opt_point$sensitivity, pch = 19, col = "red")
text(opt_point$specificity , opt_point$sensitivity ,
     paste0("Threshold = ", round(opt_point$threshold, 2),
            "\nSensitivity = ", round(opt_point$sensitivity, 2),
            "\nSpecificity = ", round(opt_point$specificity, 2)),
     pos = 2)
```
SO we can clearly see comparing the two AUC curves, oversampled data is giving better performance, as the dataset is more balanced LDA seems to do slightly better job at making classifications while also keeping in mind the increase of specificity.

Final Remarks:
We have higher specificty but overall balanced accuray increases,

Balanced accuracy = (Sensitivity + Specificity) / 2

where:

Sensitivity: The true positive rate  the percentage of positive cases the model is able to detect. Specificity: The true negative rate  the percentage of negative cases the model is able to detect.

Accuracy is the proportion of correct predictions among all predictions, regardless of the class distribution. It is a popular metric for balanced datasets where the number of positive and negative samples is roughly the same.

However, in imbalanced datasets, where one class is much more prevalent than the other, accuracy can be misleading. A model that always predicts the majority class can achieve a high accuracy score but does not perform well in identifying the minority class. In this case, balanced accuracy, also known as the average of sensitivity and specificity, is a better metric.

Balanced accuracy takes into account both the true positive rate (sensitivity) and the true negative rate (specificity) and is less affected by imbalanced datasets. It is a better metric for evaluating models in situations where the cost of false negatives (missing positive cases) and false positives (wrongly identifying negative cases) is not equal.

Optimal point on the ROC curve indicates the higest sensitivty while maintining the highest specificity.

## LDA with Principal Component

Oversampled Data
```{r}
#Perform PCA
# Select all columns except 'Exited'
pca_input <- oversampled_churn_data[, -which(names(oversampled_churn_data) == "Exited")]

# Perform PCA
pca_output <- prcomp(pca_input, scale = TRUE)

# Add 'Exited' variable back to the PCA output data
df_pca <- data.frame(pca_output$x, Exited = oversampled_churn_data$Exited)
```


```{r}
# Compute the proportion of variance explained by each principal component
pca_var <- pca_output$sdev^2 / sum(pca_output$sdev^2)

# Create a dataframe with the proportion of variance and cumulative proportion for each principal component
pca_var_df <- data.frame(Proportion = pca_var, CumulativeProportion = cumsum(pca_var))

# Create an elbow plot
library(ggplot2)
ggplot(data = pca_var_df, aes(x = 1:length(pca_var), y = Proportion)) +
  geom_point(size = 3, color = "blue") +
  geom_line(color = "blue") +
  xlab("Principal Component") +
  ylab("Proportion of Variance") +
  ggtitle("Elbow Plot for PCA") +
  theme_bw()
```


We choose 9 Principal Components
```{r}
set.seed(1)
# Define the independent variables
predictors <- c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", "PC8", "PC9")#, "PC10")

# Define the target variable
target <- "Exited"

# Define the number of folds for cross-validation
k <- 5

#Making Test Data
test_index <- createDataPartition(df_pca$Exited, p = 0.2, list = FALSE)
test_data <- df_pca[test_index, ]
train_data <- df_pca[-test_index, ]

# Create the training control object
train_control <- trainControl(method = "cv", number = k)

# Train the LDA model with cross-validation
lda_model <- train(as.formula(paste(target, "~", paste(predictors, collapse = "+"))),
                   data = train_data,
                   method = "lda",
                   trControl = train_control)

# Print the model results
print(lda_model)

# Make predictions on the test set
predictions <- predict(lda_model, newdata = test_data, type="raw")


# Calculate the accuracy
accuracy <- confusionMatrix(predictions, test_data$Exited)
print(accuracy)
#print(paste("Accuracy:", accuracy))
```




```{r}
# Make predictions on the test set
predictions <- predict(lda_model, newdata = test_data, type="prob")
# Calculate ROC curve and AUC
# Calculate the ROC curve
library(pROC)
roc_obj <- roc(test_data$Exited, predictions$X1)

# Get the optimal operating point
opt_point <- coords(roc_obj, "best", ret=c("threshold", "specificity", "sensitivity"))


# Plot the ROC curve with the optimal point
plot(roc_obj, col = "darkblue", main = "ROC Curve", print.thres = c(0.1, 0.2, 0.3, 0.4, 0.5,0.6,0.7,0.8,0.9), print.auc = TRUE)
points(opt_point$specificity, opt_point$sensitivity, pch = 19, col = "red")
text(opt_point$specificity , opt_point$sensitivity ,
     paste0("Threshold = ", round(opt_point$threshold, 2),
            "\nSensitivity = ", round(opt_point$sensitivity, 2),
            "\nSpecificity = ", round(opt_point$specificity, 2)),
     pos = 2)
```

Regular Df

```{r}
#Perform PCA


# Select all columns except 'Exited'
pca_input <- df[, -which(names(df) == "Exited")]

# Perform PCA
pca_output <- prcomp(pca_input, scale = TRUE)

# Add 'Exited' variable back to the PCA output data
df_pca <- data.frame(pca_output$x, Exited = df$Exited)
```


```{r}
# Compute the proportion of variance explained by each principal component
pca_var <- pca_output$sdev^2 / sum(pca_output$sdev^2)

# Create a dataframe with the proportion of variance and cumulative proportion for each principal component
pca_var_df <- data.frame(Proportion = pca_var, CumulativeProportion = cumsum(pca_var))

# Create an elbow plot
library(ggplot2)
ggplot(data = pca_var_df, aes(x = 1:length(pca_var), y = Proportion)) +
  geom_point(size = 3, color = "blue") +
  geom_line(color = "blue") +
  xlab("Principal Component") +
  ylab("Proportion of Variance") +
  ggtitle("Elbow Plot for PCA") +
  theme_bw()
```

```{r}

#df_pca$Exited <- factor(df_pca$Exited, levels = c("0", "1"))
#levels(df_pca$Exited) <- make.names(levels(df_pca$Exited))
# Define the independent variables
predictors <- c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", "PC8", "PC9")#, "PC10")

# Define the target variable
target <- "Exited"

# Define the number of folds for cross-validation
k <- 5

#Making Test Data
test_index <- createDataPartition(df_pca$Exited, p = 0.2, list = FALSE)
test_data <- df_pca[test_index, ]
train_data <- df_pca[-test_index, ]

# Create the training control object
train_control <- trainControl(method = "cv", number = k)

# Train the LDA model with cross-validation
lda_model <- train(as.formula(paste(target, "~", paste(predictors, collapse = "+"))),
                   data = train_data,
                   method = "lda",
                   trControl = train_control)

# Print the model results
print(lda_model)

# Make predictions on the test set
predictions <- predict(lda_model, newdata = test_data, type="raw")


# Calculate the accuracy
accuracy <- confusionMatrix(predictions, test_data$Exited)
print(accuracy)
#print(paste("Accuracy:", accuracy))
```



```{r}
# Make predictions on the test set
predictions <- predict(lda_model, newdata = test_data, type="prob")
# Calculate ROC curve and AUC
# Calculate the ROC curve
library(pROC)
roc_obj <- roc(test_data$Exited, predictions$X1)

# Get the optimal operating point
opt_point <- coords(roc_obj, "best", ret=c("threshold", "specificity", "sensitivity"))


# Plot the ROC curve with the optimal point
plot(roc_obj, col = "darkblue", main = "ROC Curve", print.thres = c(0.1, 0.2, 0.3, 0.4, 0.5,0.6,0.7,0.8,0.9), print.auc = TRUE)
points(opt_point$specificity, opt_point$sensitivity, pch = 19, col = "red")
text(opt_point$specificity , opt_point$sensitivity ,
     paste0("Threshold = ", round(opt_point$threshold, 2),
            "\nSensitivity = ", round(opt_point$sensitivity, 2),
            "\nSpecificity = ", round(opt_point$specificity, 2)),
     pos = 2)
```

# RANDOM FOREST

```{r}
library("randomForest")
```

Simple Random Forest

```{r}
# Split data into training and testing sets
set.seed(123)
trainIndex <- sample(nrow(oversampled_churn_data), 0.8 * nrow(oversampled_churn_data))
train_data <- oversampled_churn_data[trainIndex, ]
test_data <- oversampled_churn_data[-trainIndex, ]

# Train the random forest model
rf_model <- randomForest(Exited ~ ., data = train_data, keep.forest = TRUE)


# Make predictions on the test set using the random forest model
test_preds <- predict(rf_model, newdata = test_data, type = "response")

# Compute the confusion matrix
conf_matrix <- confusionMatrix(test_preds, test_data$Exited)

# Print the confusion matrix and various statistics
cat("Confusion Matrix:\n")
print(conf_matrix$table)

cat("\nAccuracy:", conf_matrix$overall["Accuracy"], "\n")
cat("Sensitivity:", conf_matrix$byClass["Sensitivity"], "\n")
cat("Specificity:", conf_matrix$byClass["Specificity"], "\n")
cat("Precision:", conf_matrix$byClass["Pos Pred Value"], "\n")
cat("Recall:", conf_matrix$byClass["Sensitivity"], "\n")
cat("F1 Score:", conf_matrix$byClass["F1"], "\n")
```
From our simple model we can infer, With a sensitivity of 90.03\%, the model successfully identified 90.03\% of churned consumers. The model accurately recognized the non-churned consumers with a specificity of 97.84\%. A accuracy of 94.05\% means that 94.05\% of all customers projected to churn by the model actually churned.
A high accuracy and F1 score, as well as a high sensitivity and specificity, indicate that the model is effective at detecting consumers who are likely to churn

We proceed to try Tuning
Hyperparameter Tuning

```{r}
#May take some time for the tuning
library(caret)
#If displays problem in factors use code below
#oversampled_churn_data$Exited <- factor(oversampled_churn_data$Exited, levels = c("0", "1"))
#levels(oversampled_churn_data$Exited) <- make.names(levels(oversampled_churn_data$Exited))


# Split data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(oversampled_churn_data$Exited, p = .8, list = FALSE)
train_data <- oversampled_churn_data[trainIndex, ]
test_data <- oversampled_churn_data[-trainIndex, ]

# Create a trainControl object for cross-validation
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE)

# Create a grid of hyperparameters to tune over
mtry_values <- seq(1, ncol(train_data) - 1, by = 1)
param_grid <- expand.grid(mtry = mtry_values)

# Perform hyperparameter tuning using 5-fold cross-validation and ROC as the metric
rf_tune <- train(Exited ~ ., data = train_data, method = "rf",
                   trControl = train_control, tuneGrid = param_grid,
                   metric = "AUC")

# Print the optimal hyperparameter and corresponding ROC value
cat("Optimal mtry:", rf_tune$bestTune$mtry, "\n")
cat("ROC:", rf_tune$results[which.max(rf_tune$results$ROC), "ROC"], "\n")

# Train a final random forest model on the full dataset using the optimal hyperparameter
final_rf_model <- train(Exited ~ ., data = oversampled_churn_data, method = "rf",
                          trControl = train_control, tuneLength = 0,
                          preProcess = c("center", "scale"), metric = "ROC",
                          tuneGrid = data.frame(mtry = rf_tune$bestTune$mtry))

# Make predictions on the test set using the final model
test_preds <- predict(final_rf_model, newdata = test_data, type = "raw")

# Calculate the accuracy on the test set
accuracy <- mean(test_preds == test_data$Exited)
cat("Test set accuracy:", accuracy, "\n")
```


```{r}
rf_tune
```


```{r}
final_rf_model
```


Train a new model based on the hyperparameters learned above
```{r}
library(caret)

# Split data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(oversampled_churn_data$Exited, p = .8, list = FALSE)
train_data <- oversampled_churn_data[trainIndex, ]
test_data <- oversampled_churn_data[-trainIndex, ]

# Train a Random Forest model using the optimal hyperparameter
rf_model <- train(Exited ~ ., data = train_data, method = "rf",
                  trControl = train_control, tuneGrid =data.frame(mtry = rf_tune$bestTune$mtry),
                  metric = "Accuracy", preProcess = c("center", "scale"),
                  tuneLength = 0)

# Make predictions on the test set using the final model
test_preds <- predict(rf_model, newdata = test_data, type = "raw")

# Compute the confusion matrix
conf_matrix <- confusionMatrix(test_preds, test_data$Exited)

# Print the confusion matrix and various statistics
cat("Confusion Matrix:\n")
print(conf_matrix$table)

cat("\nAccuracy:", conf_matrix$overall["Accuracy"], "\n")
cat("Sensitivity:", conf_matrix$byClass["Sensitivity"], "\n")
cat("Specificity:", conf_matrix$byClass["Specificity"], "\n")
cat("Precision:", conf_matrix$byClass["Pos Pred Value"], "\n")
cat("Recall:", conf_matrix$byClass["Sensitivity"], "\n")
cat("F1 Score:", conf_matrix$byClass["F1"], "\n")
```



```{r}
# Make predictions on the test set using the final model
test_preds <- predict(rf_model, newdata = test_data, type = "prob")
# Calculate ROC curve and AUC
# Calculate the ROC curve
library(pROC)
roc_obj <- roc(test_data$Exited, test_preds$X1)

# Get the optimal operating point
opt_point <- coords(roc_obj, "best", ret=c("threshold", "specificity", "sensitivity"))


# Plot the ROC curve with the optimal point
plot(roc_obj, col = "darkblue", main = "ROC Curve", print.thres = c(0.1, 0.2, 0.3, 0.4, 0.5,0.6,0.7,0.8,0.9), print.auc = TRUE)
points(opt_point$specificity, opt_point$sensitivity, pch = 19, col = "red")
text(opt_point$specificity , opt_point$sensitivity ,
     paste0("Threshold = ", round(opt_point$threshold, 2),
            "\nSensitivity = ", round(opt_point$sensitivity, 2),
            "\nSpecificity = ", round(opt_point$specificity, 2)),
     pos = 2)
```
